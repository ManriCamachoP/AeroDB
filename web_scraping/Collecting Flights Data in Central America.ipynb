{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08432e4e",
   "metadata": {},
   "source": [
    "# Web Scraping for Collecting Flights Data in Central America\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This Jupyter Notebook aims to collect data on airports in Central America using web scraping techniques. In the context of our SQL database project for airport management.\n",
    "\n",
    "Web scraping will allow us to extract relevant data from Airport Info, https://airportinfo.live/. This data will include details about the flights in Central America, such as schedules, the airline of that flight, airport of departura and arrival and many more...\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. Obtain data of flights in Central America from https://airportinfo.live/.\n",
    "2. Extract key information, such as schedule of flight, airline of the flight, and airport details.\n",
    "3. Store the collected data in a suitable format for further analysis and use in our SQL database.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "* **Requests**: The requests library allows users to make HTTP requests to the web pages they want to analyze, facilitating the download of HTML content from these pages for further processing.\n",
    "\n",
    "* **Beautiful Soup (bs4)**: Beautiful Soup is a useful tool for parsing and searching HTML elements in the downloaded content. It enables users to search and extract specific information from web pages, such as titles, paragraphs, links, and more.\n",
    "\n",
    "* **Selenium**: When websites use JavaScript to load dynamic content, Selenium becomes a valuable choice. With this library, users can automate a web browser to interact with the website and extract data from pages that require interaction.\n",
    "\n",
    "* **Pandas**: Pandas is an essential library for structuring and manipulating extracted data. It allows users to create DataFrames to organize data into rows and columns, facilitating operations such as cleaning, filtering, and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4e8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import ui\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59c2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Can not find chromedriver for currently installed chrome version.\n"
     ]
    }
   ],
   "source": [
    "chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c978a5",
   "metadata": {},
   "source": [
    "## Web Scraping for Data Collection\n",
    "\n",
    "In this section, we will explore web scraping, an automated technique for extracting data from websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a5644-079b-4e2d-bca7-24f0dcd5ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wb_driver(extension):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    driver.get(f\"https://airportinfo.live/{extension}\")\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolect_flight(extension):\n",
    "    \n",
    "    info = airport_basic(extension)[0].split()[0]\n",
    "    \n",
    "    driver = wb_driver(extension)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    hora = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"timeStart\"]')))\n",
    "\n",
    "    driver.execute_script(\"return arguments[0].scrollIntoView(true);\", hora)\n",
    "    \n",
    "    date_input = driver.find_element(By.ID, \"datepicker_input\") \n",
    "\n",
    "    date_input.click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    day_9 = driver.find_element(By.XPATH, \"//a[text()='9']\")\n",
    "    \n",
    "    day_9.click()\n",
    "    \n",
    "    for i in range(1,25):\n",
    "\n",
    "        hora_fija = driver.find_element(By.XPATH,f'//*[@id=\"timeStart\"]/option[{i}]')\n",
    "\n",
    "        hora_fija.click()\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "        boton_hora = driver.find_element(By.XPATH, '//*[@id=\"fs_div\"]/div[3]/button')\n",
    "\n",
    "        boton_hora.click()\n",
    "        \n",
    "        time.sleep(5)\n",
    "    \n",
    "        try:\n",
    "\n",
    "            vuelos = driver.find_element(By.TAG_NAME, 'tbody')\n",
    "\n",
    "            vuelos = vuelos.text.split('\\n')\n",
    "            \n",
    "            a = 0\n",
    "            \n",
    "            j = 0\n",
    "            \n",
    "            while a != len(vuelos):\n",
    "            \n",
    "                arrivo = \"NO\"\n",
    "\n",
    "                while arrivo not in [\"SCHEDULED\",\"ARRIVED\",\"IN AIR\",\n",
    "                                     'You’re eligible for compensation from this flight. Check what you’re owed for free. Check now',\n",
    "                                     \"UNKNOWN\",\"DETAILS\"]:\n",
    "\n",
    "                    arrivo = vuelos[a]\n",
    "\n",
    "                    if arrivo in [\"SCHEDULED\",\"ARRIVED\",\"IN AIR\",\n",
    "                                  \"You’re eligible for compensation from this flight. Check what you’re owed for free. Check now\",\n",
    "                                  \"UNKNOWN\",\"DETAILS\"]:\n",
    "\n",
    "                        data.append(vuelos[j:(a+1)])\n",
    "\n",
    "                        j = a+1\n",
    "\n",
    "                    a +=1\n",
    "\n",
    "                    if a == len(vuelos):\n",
    "\n",
    "                        break\n",
    "\n",
    "\n",
    "        except NoSuchElementException:\n",
    "\n",
    "            pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e344337-27b0-4c95-a1f8-cec1a80c3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all(list_airports):\n",
    "    a = 0\n",
    "    arrivos_rows = []\n",
    "    \n",
    "    for k in range(len(list_airports)):\n",
    "        data = recoleccion(list_airports[k])\n",
    "        time.sleep(5)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            if data[i][0] in ('Delay', ''):\n",
    "                if data[i][0] == 'Delay':\n",
    "                    codeshare_index = 4 if data[i][2].split(' ')[0] == \"Codeshare\" else 3\n",
    "                else:\n",
    "                    codeshare_index = 2 if data[i][0].split(' ')[0] == \"Codeshare\" else 1\n",
    "\n",
    "                if len(data[i]) <= codeshare_index + 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    if data[i][codeshare_index + 1] != '' and data[i][codeshare_index + 1][0].isdigit():\n",
    "                        arrivos_rows.append([data[i][codeshare_index - 1], df_info_aero[\"Nombre\"][k],\n",
    "                                             data[i][codeshare_index], data[i][codeshare_index + 1], data[i][-3]])\n",
    "                        a += 1\n",
    "                    else:\n",
    "                        arrivos_rows.append([data[i][codeshare_index - 1], df_info_aero[\"Nombre\"][k],\n",
    "                                             data[i][codeshare_index], data[i][codeshare_index - 1].split(' ')[-1], data[i][-3]])\n",
    "                        a += 1\n",
    "\n",
    "    arrivos.loc[a: a + len(arrivos_rows) - 1] = arrivos_rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
